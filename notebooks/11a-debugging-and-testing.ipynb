{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging and Testing\n",
    "**Based on materials contributed by Anthony Scopatz, Patrick Fuller, Katy Huff, and Rachel Slaybaugh**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is debugging?\n",
    "\n",
    "The process of debugging is one of the most crucial parts of every piece of code you will ever write. Quite simply, it is finding and correcting errors in a program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does debugging matter?\n",
    "\n",
    "Unless you're perfect, you are bound to make errors. Especially early \n",
    "on, expect to spend much more time debugging than actually coding. The process \n",
    "fits the Pareto principle - you're going to spend \\~20% of your time writing \n",
    "~80% of your code, and the other ~80% of your time will be spent screaming \n",
    "obscenities at your computer (I think that's what the Pareto principle says, \n",
    "anyway). Remember to keep calm, and **LEARN** from your mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Basics: Exceptions, Errors, and Tracebacks\n",
    "\n",
    "When your code errors, Python will stop and return an *exception* that attempts \n",
    "to tell you what's up. There are approximately 165 exceptions in the Python standard \n",
    "library, and you'll be seeing many of them very soon. Exceptions to know\n",
    "include:\n",
    "\n",
    "    SyntaxError # You're probably missing a parenthesis or colon\n",
    "    NameError   # There's probably a variable name typo somewhere\n",
    "    TypeError   # You're doing something with incompatible variable types\n",
    "    ValueError  # You're calling a function with the wrong parameter\n",
    "    IOError     # You're trying to use a file that doesn't exist\n",
    "    IndexError  # You're trying to reference a list element that doesn't exist\n",
    "    KeyError    # Similar to an IndexError, but for dictionaries\n",
    "    Exception   # This means \"an error of any type\" - hopefully you don't see it often\n",
    "\n",
    "When code returns an exception, we say that the exception was **thrown** or\n",
    "**raised**. These exceptions may be **handled** or **caught** by the code. Speaking\n",
    "of, you can handle exceptions in Python like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going from zero to hero.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    a = 1.0 / 0.0\n",
    "except ZeroDivisionError:\n",
    "    print(\"Going from zero to hero.\")\n",
    "    a = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That being said, there are some things you should keep in mind.\n",
    "\n",
    "* Exception handling in your own code should be seen as a last resort. _Never_\n",
    "  use exception handling where another approach would work just as well.\n",
    "* If you have to handle exceptions, be specific in their type. Writing a blanket\n",
    "  `except Exception` line provides a place for unintended bugs to hide.\n",
    "\n",
    "When an exception is printed, it often comes with something called a\n",
    "**traceback**. This is Python's attempt to tell you where the code errored. It\n",
    "will look like gibberish for a while, but that impression will go away with time.\n",
    "\n",
    "So, when your code errors, Python tells you \n",
    "\n",
    "1. why it errored, and \n",
    "2. where it errored. \n",
    "\n",
    "\"*Isn't that enough to debug?*\", you might ask. \n",
    "\n",
    "Well, yeah. It is.  However if you debug only by running your code you will be spending a lot more\n",
    "time in the screaming-obscenities-at-your-computer portion of coding. Every tool\n",
    "discussed below doesn't add much in terms of functionality (they're still just\n",
    "pointing out errors), but they all help in decreasing debugging time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linting: Catching the Stupid Errors\n",
    "\n",
    "As I said before, you can debug by simply attempting to run your code. This,\n",
    "however, is very annoying. First off, the code will always stop at the first \n",
    "exception. This means that, if you have ten errors, you'll have to run the code \n",
    "ten times to find all of them. Now, imagine that this is long-running code.\n",
    "Imagine waiting five minutes for your code to run, only to discover it breaks\n",
    "because of a typo. Doesn't that sound terrible?\n",
    "\n",
    "Enter linting. \"Linting\" is the process of discovering errors in a code (typically \n",
    "typos and syntax errors --- i.e., the dumb stuff) before the code is ever run or \n",
    "compiled. In Python, this can be accomplished through using the \n",
    "[pyflakes](http://pypi.python.org/pypi/pyflakes/) \n",
    "library. It works by statically analyzing your code without running it. This \n",
    "means that it can find multiple errors at once, rather than stopping at the \n",
    "first exception.\n",
    "\n",
    "You can run pyflakes on your code by typing:\n",
    "\n",
    "    $ pyflakes my_code.py\n",
    "    \n",
    "We can take this a step further with integrated development environments, or\n",
    "*IDEs*. IDEs are (basically) glorified text editors that dynamically lint,\n",
    "showing you typos as you write your code. You will find that coders generally\n",
    "have strong opinions on the use of IDEs, either positive or negative. Regardless,\n",
    "if you want to play around with one, I recommend [Eclipse](http://www.eclipse.org/) \n",
    "with the [PyDev](http://pydev.org/) plugin, or [Visual Studo Code](https://code.visualstudio.com/docs) with the Python extension in its extension marketplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding standards: The Details Matter!\n",
    "\n",
    "> The one skill that separates bad programmers from good programmers is attention to detail. \n",
    ">\n",
    "> Zed Shaw, _Learn Python the Hard Way_\n",
    "\n",
    "In a written natural language, there are many ways to express the same idea. To \n",
    "make the consumption of information easier, people define style guides to enforce \n",
    "particularly effective ways of writing. This is even more true in coding; \n",
    "consistent style choices make scripts much easier to read. They become absolutely \n",
    "essential as projects become large (>1 person).\n",
    "\n",
    "Some programming languages, such as Java and C++,  have multiple competing standards, \n",
    "and it's easy to imagine how messy this can get. Luckily, Python doesn't have \n",
    "this issue. The official standard, [PEP8](http://www.python.org/dev/peps/pep-0008/), \n",
    "is used everywhere. Unless you plan on hiding all the code you write from the \n",
    "outside world you should learn PEP8.\n",
    "\n",
    "To help out coders, there are tools to test for compliance. The aptly named \n",
    "`pep8` library shows you where your code deviates from the PEP8 standard, and\n",
    "`autopep8` goes a step further by trying to fix all of your errors for you.\n",
    "These are both run from the shell, as\n",
    "\n",
    "\n",
    "    $ pep8 my_code.py\n",
    "    \n",
    "    $ autopep8 my_code.py > my_new_code.py\n",
    "\n",
    "These libraries won't always pick up everything. Furthermore, due to\n",
    "the desire to maintain backward compatibility, there is some wiggle room in PEP8 \n",
    "(see [this powerpoint](www.python.org/doc/essays/ppt/regrets/PythonRegrets.ppt) \n",
    "of Python regrets, made by the creator of the language). Here are some additional\n",
    "rules to remember:\n",
    "\n",
    "**PEP8 conventions missed by the `pep8` checker:**\n",
    "\n",
    "* Variables and functions should be named  in `snake_case`. No capital letters.\n",
    "  Classes are named in `CapCase`.\n",
    "* Multiline comments use `\"\"\"`, not `'''`.\n",
    "* Private methods and variables should be prefixed with an underscore, ie. \n",
    "  `_my_private_method()`.\n",
    " \n",
    "**Special rules outside of PEP8**\n",
    "\n",
    "* _Never_ use tabs. Ever.\n",
    "* Use list comprehensions over `map()`, `reduce()`, and `filter()`.\n",
    "* Avoid iterating through lists by index whenever possible.\n",
    " \n",
    "These rules might seem arbitrary at first.  However, they\n",
    "make collaborative coding much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debuggers: for the deep-rooted errors\n",
    "\n",
    "Linting will only catch the really obvious errors. For more complex issues,\n",
    "(ie. bugs), you're going to want to follow the code's logic line by line. One\n",
    "lazy way to do this is to put `print` statements everywhere, which allows you\n",
    "to view variables over time. However, this gets messy quickly, and you lose\n",
    "control of what variables you can see once you start executing.\n",
    "\n",
    "This is where the Python DeBugger, or _pdb_, comes into play. With it, you can \n",
    "step through your code and watch as variables are changed.\n",
    "\n",
    "All you have to do to use this is import the `pdb` module and call the \n",
    "`set_trace()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "# ... \n",
    "# Your code here\n",
    "# ...\n",
    "pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when you run the code, it will stop at whatever line you put `set_trace()`.\n",
    "You'll be prompted to give a command. Some common commands include:\n",
    "\n",
    " * `continue` continues on to the next time a `set_trace()` line is hit\n",
    " * `print <variable>` prints the current value of a specified variable\n",
    " * `list` shows the source code around the `set_trace()` line\n",
    " * `args` prints the values of all the arguments in the current function\n",
    "\n",
    "There are a lot more options, which can be found [here](http://docs.python.org/2/library/pdb.html), \n",
    "but these few should be enough to get you running with pdb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Profiling: making code fast\n",
    "\n",
    "So, you've found your errors, those deep-rooted bugs, and even standardized\n",
    "your code to conform to PEP8. But, for some reason, it's still really slow.\n",
    "What can we do about this?\n",
    "\n",
    "The first idea you might have is to time your code. Analogous to the `print`\n",
    "statement debugging above, you could write some logic to print run times at\n",
    "various points in your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "t_naught = time()\n",
    "# code you want to  time\n",
    "print(time() - t_naught)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also time your entire script with the `time` BASH command:\n",
    "\n",
    "    $ time python my_code.py\n",
    "\n",
    "While these both work, they're either too messy or not detailed enough. What we\n",
    "really want is a breakdown of how long the computer spends running each part\n",
    "of our code.\n",
    "\n",
    "*Profilers* provide a way to do just this. With Python, run your script in a\n",
    "shell with this command\n",
    "\n",
    "    $ python -m cProfile -s time my_code.py\n",
    "\n",
    "This returns the amount of functions called in the execution, along with a\n",
    "breakdown of the time each function took. The `-s time` part sorts the output\n",
    "by the time taken (which is usually what you care about). A sample output looks \n",
    "like:\n",
    "\n",
    "    2530004 function calls in 0.789 seconds\n",
    "    Ordered by: internal time\n",
    "   \n",
    "     ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
    "    1980000    0.253    0.000    0.253    0.000 my_code.py:23(&lt;genexpr&gt;)\n",
    "      10000    0.190    0.000    0.780    0.000 my_code.py:16(evolve)\n",
    "     220000    0.182    0.000    0.436    0.000 {sum}\n",
    "     270000    0.150    0.000    0.150    0.000 my_code.py:28(neighbors)\n",
    "          1    0.009    0.009    0.789    0.789 my_code.py:9(my_func)\n",
    "      50000    0.004    0.000    0.004    0.000 {method 'add' of 'set' objects}\n",
    "          1    0.000    0.000    0.000    0.000 {range}\n",
    "\n",
    "With this information, you can go back into your code and adjust the logic to \n",
    "improve the speed bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing: For making sure it does what you think\n",
    "\n",
    "Software testing is a process by which one or more expected behaviors\n",
    "and results from a piece of software are exercised and confirmed. Well\n",
    "chosen tests will confirm expected code behavior for the extreme\n",
    "boundaries of the input domains, output ranges, parametric combinations,\n",
    "and other behavioral **edge cases**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why test software?\n",
    "\n",
    "Unless you write flawless, bug-free, perfectly accurate, fully precise,\n",
    "and predictable code *every time*, you must test your code in order to\n",
    "trust it enough to answer in the affirmative to at least a few of the\n",
    "following questions:\n",
    "\n",
    "-  Does your code work?\n",
    "-  **Always?**\n",
    "-  Does it do what you think it does? ([Patriot Missile Failure](http://www.ima.umn.edu/~arnold/disasters/patriot.html))\n",
    "-  Does it continue to work after changes are made?\n",
    "-  Does it continue to work after system configurations or libraries\n",
    "   are upgraded?\n",
    "-  Does it respond properly for a full range of input parameters?\n",
    "-  What about **edge and corner cases**?\n",
    "-  What's the limit on that input parameter?\n",
    "-  How will it affect your\n",
    "   [publications](http://www.nature.com/news/2010/101013/full/467775a.html)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification\n",
    "\n",
    "*Verification* is the process of asking, \"Have we built the software\n",
    "correctly?\" That is, is the code bug free, precise, accurate, and\n",
    "repeatable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "*Validation* is the process of asking, \"Have we built the right\n",
    "software?\" That is, is the code designed in such a way as to produce the\n",
    "answers we are interested in, data we want, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty Quantification\n",
    "\n",
    "*Uncertainty Quantification* is the process of asking, \"Given that our\n",
    "algorithm may not be deterministic, was our execution within acceptable\n",
    "error bounds?\" This is particularly important for anything which uses\n",
    "random numbers, eg Monte Carlo methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are tests?\n",
    "\n",
    "Say we have an averaging function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(numlist):\n",
    "    total = sum(numlist)\n",
    "    length = len(numlist)\n",
    "    return total/length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests could be implemented as runtime *exceptions in the function*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(numlist):\n",
    "    try:\n",
    "        total = sum(numlist)\n",
    "        length = len(numlist)\n",
    "    except TypeError:\n",
    "        raise TypeError(\"The number list was not a list of numbers.\")\n",
    "    except:\n",
    "        print(\"There was a problem evaluating the number list.\")\n",
    "    return total/length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes tests are functions alongside the function definitions\n",
    "they are testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(numlist):\n",
    "    try:\n",
    "        total = sum(numlist)\n",
    "        length = len(numlist)\n",
    "    except TypeError:\n",
    "        raise TypeError(\"The number list was not a list of numbers.\")\n",
    "    except:\n",
    "        print(\"There was a problem evaluating the number list.\")\n",
    "    return total/length\n",
    "\n",
    "def test_mean():\n",
    "    assert mean([0, 0, 0, 0]) == 0\n",
    "    assert mean([0, 200]) == 100\n",
    "    assert mean([0, -200]) == -100\n",
    "    assert mean([0]) == 0\n",
    "\n",
    "\n",
    "def test_floating_mean():\n",
    "    assert mean([1, 2]) == 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes tests live in an executable independent of the main executable.\n",
    "\n",
    "**Implementation File:** `mean.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(numlist):\n",
    "    try:\n",
    "        total = sum(numlist)\n",
    "        length = len(numlist)\n",
    "    except TypeError:\n",
    "        raise TypeError(\"The number list was not a list of numbers.\")\n",
    "    except:\n",
    "        print(\"There was a problem evaluating the number list.\")\n",
    "    return total/length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test File:** `test_mean.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mean import mean\n",
    "\n",
    "def test_mean():\n",
    "    assert mean([0, 0, 0, 0]) == 0\n",
    "    assert mean([0, 200]) == 100\n",
    "    assert mean([0, -200]) == -100\n",
    "    assert mean([0]) == 0\n",
    "\n",
    "\n",
    "def test_floating_mean():\n",
    "    assert mean([1, 2]) == 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When should we test?\n",
    "\n",
    "The three right answers are:\n",
    "\n",
    "- **ALWAYS!**\n",
    "- **EARLY!**\n",
    "- **OFTEN!**\n",
    "\n",
    "The longer answer is that testing either before or after your software\n",
    "is written will improve your code, but testing after your program is\n",
    "used for something important is too late.\n",
    "\n",
    "If we have a robust set of tests, we can run them before adding\n",
    "something new and after adding something new. If the tests give the same\n",
    "results (as appropriate), we can have some assurance that we didn't\n",
    "wreak anything. The same idea applies to making changes in your system\n",
    "configuration, updating support codes, etc.\n",
    "\n",
    "Another important feature of testing is that it helps you remember what\n",
    "all the parts of your code do. If you are working on a large project\n",
    "over three years and you end up with 200 classes, it may be hard to\n",
    "remember what the widget class does in detail. If you have a test that\n",
    "checks all of the widget's functionality, you can look at the test to\n",
    "remember what it's supposed to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who should test?\n",
    "\n",
    "In a collaborative coding environment, where many developers contribute\n",
    "to the same code base, developers should be responsible individually for\n",
    "testing the functions they create and collectively for testing the code\n",
    "as a whole.\n",
    "\n",
    "Professionals often test their code, and take pride in test coverage,\n",
    "the percent of their functions that they feel confident are\n",
    "comprehensively tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are tests written?\n",
    "\n",
    "The type of tests that are written is determined by the testing\n",
    "framework you adopt. Don't worry, there are a lot of choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Tests\n",
    "\n",
    "**Exceptions:** Exceptions can be thought of as type of runtime test.\n",
    "They alert the user to exceptional behavior in the code. Often,\n",
    "exceptions are related to functions that depend on input that is unknown\n",
    "at compile time. Checks that occur within the code to handle exceptional\n",
    "behavior that results from this type of input are called Exceptions.\n",
    "\n",
    "**Unit Tests:** Unit tests are a type of test which test the fundamental\n",
    "units of a program's functionality. Often, this is on the class or\n",
    "function level of detail. However what defines a *code unit* is not\n",
    "formally defined.\n",
    "\n",
    "To test functions and classes, the interfaces (API) - rather than the\n",
    "implementation - should be tested. Treating the implementation as a\n",
    "black box, we can probe the expected behavior with boundary cases for\n",
    "the inputs.\n",
    "\n",
    "**System Tests:** System level tests are intended to test the code as a\n",
    "whole. As opposed to unit tests, system tests ask for the behavior as a\n",
    "whole. This sort of testing involves comparison with other validated\n",
    "codes, analytical solutions, etc.\n",
    "\n",
    "**Regression Tests:** A regression test ensures that new code does\n",
    "change anything. If you change the default answer, for example, or add a\n",
    "new question, you'll need to make sure that missing entries are still\n",
    "found and fixed.\n",
    "\n",
    "**Integration Tests:** Integration tests query the ability of the code\n",
    "to integrate well with the system configuration and third party\n",
    "libraries and modules. This type of test is essential for codes that\n",
    "depend on libraries which might be updated independently of your code or\n",
    "when your code might be used by a number of users who may have various\n",
    "versions of libraries.\n",
    "\n",
    "**Test Suites:** Putting a series of unit tests into a collection of\n",
    "modules creates, a test suite. Typically the suite as a whole is\n",
    "executed (rather than each test individually) when verifying that the\n",
    "code base still functions after changes have been made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements of a Test\n",
    "\n",
    "**Behavior:** The behavior you want to test. For example, you might want\n",
    "to test the fun() function.\n",
    "\n",
    "**Expected Result:** This might be a single number, a range of numbers,\n",
    "a new fully defined object, a system state, an exception, etc. When we\n",
    "run the fun() function, we expect to generate some fun. If we don't\n",
    "generate any fun, the fun() function should fail its test.\n",
    "Alternatively, if it does create some fun, the fun() function should\n",
    "pass this test. The the expected result should known *a priori*. For\n",
    "numerical functions, this is result is ideally analytically determined\n",
    "even if the function being tested isn't.\n",
    "\n",
    "**Assertions:** Require that some conditional be true. If the\n",
    "conditional is false, the test fails.\n",
    "\n",
    "**Fixtures:** Sometimes you have to do some legwork to create the\n",
    "objects that are necessary to run one or many tests. These objects are\n",
    "called fixtures as they are not really part of the test themselves but\n",
    "rather involve getting the computer into the appropriate state.\n",
    "\n",
    "For example, since fun varies a lot between people, the fun() function\n",
    "is a method of the Person class. In order to check the fun function,\n",
    "then, we need to create an appropriate Person object on which to run\n",
    "fun().\n",
    "\n",
    "**Setup and teardown:** Creating fixtures is often done in a call to a\n",
    "setup function. Deleting them and other cleanup is done in a teardown\n",
    "function.\n",
    "\n",
    "**The Big Picture:** Putting all this together, the testing algorithm is\n",
    "often:\n",
    "\n",
    "    setup()\n",
    "    test()\n",
    "    teardown()\n",
    "\n",
    "But, sometimes it's the case that your tests change the fixtures. If so,\n",
    "it's better for the setup() and teardown() functions to occur on either\n",
    "side of each test. In that case, the testing algorithm should be:\n",
    "\n",
    "    setup()\n",
    "    test1()\n",
    "    teardown()\n",
    "\n",
    "    setup()\n",
    "    test2()\n",
    "    teardown()\n",
    "\n",
    "    setup()\n",
    "    test3()\n",
    "    teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nose: A Python Testing Framework\n",
    "\n",
    "The testing framework we'll discuss today is called nose. However, there are\n",
    "several other testing frameworks available in most language. Most notably there\n",
    "is [JUnit](http://www.junit.org/) in Java which can arguably attributed to\n",
    "inventing the testing framework. Google also provides a [test\n",
    "framework](code.google.com/p/googletest/) for C++ applications (note, there's\n",
    "also [CTest](http://cmake.org/Wiki/CMake/Testing_With_CTest)).  There\n",
    "is at least one testing framework for R:\n",
    "[testthat](http://cran.r-project.org/web/packages/testthat/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where do nose tests live?\n",
    "\n",
    "Nose tests are files that begin with `Test-`, `Test_`, `test-`, or\n",
    "`test_`. Specifically, these satisfy the testMatch regular expression\n",
    "`[Tt]est[-_]`. (You can also teach nose to find tests by declaring them\n",
    "in the unittest.TestCase subclasses chat you create in your code. You\n",
    "can also create test functions which are not unittest.TestCase\n",
    "subclasses if they are named with the configured testMatch regular\n",
    "expression.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nose Test Syntax\n",
    "\n",
    "To write a nose test, we make assertions.\n",
    "\n",
    "    assert should_be_true()\n",
    "    assert not should_not_be_true()\n",
    "\n",
    "Additionally, nose itself defines number of assert functions which can\n",
    "be used to test more specific aspects of the code base.\n",
    "\n",
    "    from nose.tools import *\n",
    "\n",
    "    assert_equal(a, b)\n",
    "    assert_almost_equal(a, b)\n",
    "    assert_true(a)\n",
    "    assert_false(a)\n",
    "    assert_raises(exception, func, *args, **kwargs)\n",
    "    assert_is_instance(a, b)\n",
    "    # and many more!\n",
    "\n",
    "Moreover, numpy offers similar testing functions for arrays:\n",
    "\n",
    "    from numpy.testing import *\n",
    "\n",
    "    assert_array_equal(a, b)\n",
    "    assert_array_almost_equal(a, b)\n",
    "    # etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Writing tests for mean()\n",
    "\n",
    "There are a few tests for the mean() function that we listed in this\n",
    "lesson. What are some tests that should fail? Add at least three test\n",
    "cases to this set. Edit the `test_mean.py` file which tests the mean()\n",
    "function in `mean.py`.\n",
    "\n",
    "*Hint:* Think about what form your input could take and what you should\n",
    "do to handle it. Also, think about the type of the elements in the list.\n",
    "What should be done if you pass a list of integers? What if you pass a\n",
    "list of strings?\n",
    "\n",
    "**Example**:\n",
    "\n",
    "    $ nosetests test_mean.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Driven Development\n",
    "\n",
    "Test driven development (TDD) is a philosophy whereby the developer\n",
    "creates code by **writing the tests first**. That is to say you write the\n",
    "tests *before* writing the associated code!\n",
    "\n",
    "This is an iterative process whereby you write a test then write the\n",
    "minimum amount code to make the test pass. If a new feature is needed,\n",
    "another test is written and the code is expanded to meet this new use\n",
    "case. This continues until the code does what is needed.\n",
    "\n",
    "TDD operates on the YAGNI principle (You Ain't Gonna Need It). People\n",
    "who diligently follow TDD swear by its effectiveness. This development\n",
    "style was put forth most strongly by [Kent Beck in\n",
    "2002](http://www.amazon.com/Test-Driven-Development-By-Example/dp/0321146530)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A TDD Example\n",
    "\n",
    "Say you want to write a std() function which computes the [Standard \n",
    "Deviation](http://en.wikipedia.org/wiki/Standard_deviation). You\n",
    "would - of course - start by writing the test, possibly testing a single set of \n",
    "numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_almost_equal\n",
    "\n",
    "def test_std1():\n",
    "    obs = std([0.0, 2.0])\n",
    "    exp = 1.0\n",
    "    assert_equal(obs, exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would *then* go ahead and write the actual function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(vals):\n",
    "    # you snarky so-and-so\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is it, right?! Well, not quite. This implementation fails for\n",
    "most other values. Adding tests we see that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_std1():\n",
    "    obs = std([0.0, 2.0])\n",
    "    exp = 1.0\n",
    "    assert_equal(obs, exp)\n",
    "\n",
    "def test_std2():\n",
    "    obs = std([])\n",
    "    exp = 0.0\n",
    "    assert_equal(obs, exp)\n",
    "\n",
    "def test_std3():\n",
    "    obs = std([0.0, 4.0])\n",
    "    exp = 2.0\n",
    "    assert_equal(obs, exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These extra tests now require that we bother to implement at least a slightly more reasonable function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(vals):\n",
    "    # a little better\n",
    "    if len(vals) == 0:\n",
    "        return 0.0\n",
    "    return vals[-1] / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this function still fails whenever vals has more than two elements or\n",
    "the first element is not zero. Time for more tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_std1():\n",
    "    obs = std([0.0, 2.0])\n",
    "    exp = 1.0\n",
    "    assert_equal(obs, exp)\n",
    "\n",
    "def test_std2():\n",
    "    obs = std([])\n",
    "    exp = 0.0\n",
    "    assert_equal(obs, exp)\n",
    "\n",
    "def test_std3():\n",
    "    obs = std([0.0, 4.0])\n",
    "    exp = 2.0\n",
    "    assert_equal(obs, exp)\n",
    "\n",
    "def test_std4():\n",
    "    obs = std([1.0, 3.0])\n",
    "    exp = 1.0\n",
    "    assert_equal(obs, exp)\n",
    "\n",
    "def test_std5():\n",
    "    obs = std([1.0, 1.0, 1.0])\n",
    "    exp = 0.0\n",
    "    assert_equal(obs, exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we had better go ahead and try do the right thing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(vals):\n",
    "    # finally, some math\n",
    "    n = len(vals)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    mu = sum(vals) / n\n",
    "    var = 0.0\n",
    "    for val in vals:\n",
    "        var = var + (val - mu)**2\n",
    "    return (var / n)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it becomes very tempting to take an extended coffee break or\n",
    "possibly a power lunch. But then you remember those pesky infinite values!\n",
    "Perhaps the right thing to do here is to just be undefined.  Infinity in \n",
    "Python may be represented by any literal float greater than or equal to 1e309."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_std1():\n",
    "    obs = std([0.0, 2.0])\n",
    "    exp = 1.0\n",
    "    assert_equal(obs, exp)\n",
    "\n",
    "def test_std2():\n",
    "    obs = std([])\n",
    "    exp = 0.0\n",
    "    assert_equal(obs, exp)\n",
    "\n",
    "def test_std3():\n",
    "    obs = std([0.0, 4.0])\n",
    "    exp = 2.0\n",
    "    assert_equal(obs, exp)\n",
    "\n",
    "def test_std4():\n",
    "    obs = std([1.0, 3.0])\n",
    "    exp = 1.0\n",
    "    assert_equal(obs, exp)\n",
    "\n",
    "def test_std5():\n",
    "    obs = std([1.0, 1.0, 1.0])\n",
    "    exp = 0.0\n",
    "    assert_equal(obs, exp)\n",
    "\n",
    "def test_std6():\n",
    "    obs = std([1e500])\n",
    "    exp = NotImplemented\n",
    "    assert_equal(obs, exp)\n",
    "\n",
    "def test_std7():\n",
    "    obs = std([0.0, 1e4242])\n",
    "    exp = NotImplemented\n",
    "    assert_equal(obs, exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that it is time to add the appropriate case to the function\n",
    "itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(vals):\n",
    "    # sequence and you shall find\n",
    "    n = len(vals)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    mu = sum(vals) / n\n",
    "    if mu == 1e500:\n",
    "        return NotImplemented\n",
    "    var = 0.0\n",
    "    for val in vals:\n",
    "        var = var + (val - mu)**2\n",
    "    return (var / n)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Statistics Tests\n",
    "The `stats.py` and `test_stats.py` files in the `/notebooks/OtherFiles/` folder contain stubs for other simple statistics functions. Try your new test-driven development chops by implementing one or more of these functions along with their corresponding tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Excercise\n",
    "\n",
    "Complete the files in `/OtherFiles/` called `random_thousand.py` and `test_random_thousand.py`. Make sure you are using test driven development by writing your tests first! These tests will require you to decide on and read documentation for several modules. View this as also an excercise in your googling abilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
